{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.52.4 pyngrok fastapi uvicorn python-multipart openai-whisper pypdf ffmpeg-python --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, torch, threading, time, socket, tempfile, logging, re\n",
    "from pyngrok import ngrok, conf\n",
    "from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Depends\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import whisper\n",
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "NGROK_TOKEN = \"Put your Second ngrok token here\"\n",
    "API_KEY = \"123456\"\n",
    "TEMP_DIR = tempfile.mkdtemp()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Models:\n",
    "    mistral_model = None\n",
    "    mistral_tokenizer = None\n",
    "    whisper_model = None\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        logger.info(\"ðŸ”„ Loading Kaggle models...\")\n",
    "\n",
    "        # Load Mistral\n",
    "        logger.info(\"Loading Mistral...\")\n",
    "        cls.mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "        cls.mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        ).eval()\n",
    "        logger.info(\"âœ… Mistral loaded\")\n",
    "\n",
    "        # Load Whisper (tiny to avoid GPU crash)\n",
    "        logger.info(\"Loading Whisper...\")\n",
    "        cls.whisper_model = whisper.load_model(\"tiny\")\n",
    "        logger.info(\"âœ… Whisper loaded\")\n",
    "\n",
    "        logger.info(\"âœ… All Kaggle models ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECURITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "security = HTTPBearer()\n",
    "\n",
    "def verify(creds: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    if creds.credentials != API_KEY:\n",
    "        raise HTTPException(401, \"Unauthorized\")\n",
    "    return creds.credentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT PARSER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def parse_model_output(raw_output: str, prompt: str, analysis_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse and clean the model output to remove prompt repetition and extract only the response.\n",
    "    \"\"\"\n",
    "    # Remove the prompt from the output if it appears at the beginning\n",
    "    output = raw_output\n",
    "    \n",
    "    # Try to find where the actual response starts\n",
    "    # Common markers that indicate the start of the actual response\n",
    "    markers = {\n",
    "        \"brief\": [\"Provide a brief summary\", \"Brief summary:\", \"Summary:\"],\n",
    "        \"detailed\": [\"Detailed Summary:\", \"Detailed Analysis:\", \"Analysis:\"],\n",
    "        \"main_topics\": [\"Main Topics:\", \"Topics:\"],\n",
    "        \"keywords_focus\": [\"Keyword-Focused Analysis:\", \"Analysis:\"],\n",
    "        \"bullet_points\": [\"Key Points:\", \"â€¢\", \"-\"],\n",
    "        \"question_answer\": [\"Q&A Summary:\", \"Q:\", \"Question:\"]\n",
    "    }\n",
    "    \n",
    "    # Find the last occurrence of the marker for this analysis type\n",
    "    relevant_markers = markers.get(analysis_type, [\"Analysis:\", \"Summary:\"])\n",
    "       \n",
    "    # Try each marker\n",
    "    for marker in relevant_markers:\n",
    "        if marker in output:\n",
    "            # Split at the last occurrence of the marker\n",
    "            parts = output.split(marker)\n",
    "            if len(parts) > 1:\n",
    "                # Take everything after the last marker\n",
    "                output = parts[-1].strip()\n",
    "                break\n",
    "    \n",
    "    # Remove the prompt text if it's still there\n",
    "    prompt_lines = prompt.split('\\n')\n",
    "    for line in prompt_lines:\n",
    "        if len(line.strip()) > 20:  # Only remove substantial lines\n",
    "            output = output.replace(line, '')\n",
    "    \n",
    "    # Remove common instruction patterns using regex\n",
    "    patterns_to_remove = [\n",
    "        r'User Keywords:.*?TEXT:',\n",
    "        r'Provide a brief summary.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Provide a detailed summary.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Identify and explain.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Create a Q&A summary.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Analyze the text focusing.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Extract and explain.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'For each topic:.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Each bullet should be.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'\\*\\s*Topic name.*?(?=\\n)',\n",
    "        r'\\*\\s*Brief explanation.*?(?=\\n)',\n",
    "        r'\\*\\s*Key points.*?(?=\\n)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        output = re.sub(pattern, '', output, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Clean up extra whitespace and newlines\n",
    "    output = re.sub(r'\\n{3,}', '\\n\\n', output)\n",
    "    output = output.strip()\n",
    "    \n",
    "    # If output is too short or empty, return a fallback message\n",
    "    if len(output) < 50:\n",
    "        return \"Analysis generated. Please note: The model output was too brief or could not be properly parsed.\"\n",
    "    \n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "async def save_file(file: UploadFile, prefix: str) -> str:\n",
    "    path = os.path.join(TEMP_DIR, f\"{prefix}{Path(file.filename).suffix}\")\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(await file.read())\n",
    "    return path\n",
    "\n",
    "def extract_pdf(path: str) -> str:\n",
    "    return \"\\n\".join([p.extract_text() or \"\" for p in PdfReader(path).pages])\n",
    "\n",
    "def transcribe(path: str) -> str:\n",
    "    return Models.whisper_model.transcribe(path)[\"text\"]\n",
    "\n",
    "def extract_audio_from_video(video_path: str, audio_path: str):\n",
    "    import subprocess\n",
    "    subprocess.run([\n",
    "        'ffmpeg', '-i', video_path, '-vn', '-acodec',\n",
    "        'libmp3lame', '-q:a', '2', audio_path, '-y'\n",
    "    ], check=True, capture_output=True)\n",
    "\n",
    "def generate_analysis(prompt: str, max_len: int = 400) -> str:\n",
    "    inputs = Models.mistral_tokenizer(\n",
    "        prompt, return_tensors=\"pt\",\n",
    "        truncation=True, max_length=2048\n",
    "    ).to(Models.mistral_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = Models.mistral_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_len,\n",
    "            pad_token_id=Models.mistral_tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    return Models.mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def build_analysis_prompt(text: str, analysis_type: str, keywords: str = \"\") -> tuple:\n",
    "    \"\"\"Build analysis prompt based on type and return (prompt, max_tokens)\"\"\"\n",
    "    \n",
    "    text_preview = text[:3000]  # Limit text for context\n",
    "    keyword_text = f\"Focus Keywords: {keywords}\" if keywords else \"No specific keywords provided\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"brief\": (f\"\"\"You are a summarization expert. Analyze this text and provide ONLY a brief 2-3 sentence summary.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Write your brief summary below (2-3 sentences only):\n",
    "\"\"\", 200),\n",
    "        \n",
    "        \"detailed\": (f\"\"\"You are an analysis expert. Provide a comprehensive analysis of this text.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Write a detailed analysis covering main themes, key points, evidence, and conclusions:\n",
    "\"\"\", 600),\n",
    "        \n",
    "        \"main_topics\": (f\"\"\"You are a content analyzer. Extract the main topics from this text.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "List and explain each main topic with its key points:\n",
    "\"\"\", 500),\n",
    "        \n",
    "        \"keywords_focus\": (f\"\"\"You are an expert analyzer. Focus your analysis on these specific keywords: {keywords}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Analyze how these keywords appear in the text, their context, and significance:\n",
    "\"\"\", 500),\n",
    "        \n",
    "        \"bullet_points\": (f\"\"\"You are a summarization expert. Extract the key points as clear bullet points.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "List the main points as bullet points:\n",
    "\"\"\", 400),\n",
    "        \n",
    "        \"question_answer\": (f\"\"\"You are an expert analyzer. Create a Q&A format summary.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Create questions and answers about this text:\n",
    "\"\"\", 500)\n",
    "    }\n",
    "    \n",
    "    return prompts.get(analysis_type, prompts[\"detailed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTAPI APP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Kaggle Enhanced Analysis API\", version=\"2.0\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    Models.load()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"status\":\"online\", \"version\": \"2.0\", \"features\": [\"extraction\", \"multi-mode analysis\", \"output parser\"]}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"models\": {\n",
    "            \"mistral\": Models.mistral_model is not None,\n",
    "            \"whisper\": Models.whisper_model is not None\n",
    "        },\n",
    "        \"analysis_modes\": [\"brief\", \"detailed\", \"main_topics\", \"keywords_focus\", \"bullet_points\", \"question_answer\"]\n",
    "    }\n",
    "\n",
    "@app.post(\"/extract\")\n",
    "async def extract(\n",
    "    input_type: str = Form(...),\n",
    "    file: UploadFile = File(...),\n",
    "    token: str = Depends(verify)\n",
    "):\n",
    "    file_path = audio_path = None\n",
    "\n",
    "    try:\n",
    "        if input_type == \"pdf\":\n",
    "            file_path = await save_file(file, \"doc\")\n",
    "            text = extract_pdf(file_path)\n",
    "\n",
    "        elif input_type == \"audio\":\n",
    "            file_path = await save_file(file, \"audio\")\n",
    "            text = transcribe(file_path)\n",
    "\n",
    "        elif input_type == \"video\":\n",
    "            file_path = await save_file(file, \"video\")\n",
    "            audio_path = os.path.join(TEMP_DIR, \"audio.mp3\")\n",
    "            extract_audio_from_video(file_path, audio_path)\n",
    "            text = transcribe(audio_path)\n",
    "\n",
    "        else:\n",
    "            raise HTTPException(400, \"Invalid input_type\")\n",
    "\n",
    "        return {\"status\":\"success\", \"text\":text, \"length\": len(text)}\n",
    "\n",
    "    finally:\n",
    "        for p in [file_path, audio_path]:\n",
    "            if p and os.path.exists(p):\n",
    "                os.remove(p)\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze(\n",
    "    text: str = Form(...),\n",
    "    analysis_type: str = Form(\"detailed\"),\n",
    "    keywords: str = Form(\"\"),\n",
    "    token: str = Depends(verify)\n",
    "):\n",
    "    \"\"\"Enhanced analysis with multiple modes and output parsing\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Analysis request - Type: {analysis_type}, Keywords: {keywords}\")\n",
    "        \n",
    "        # Validate analysis type\n",
    "        valid_types = [\"brief\", \"detailed\", \"main_topics\", \"keywords_focus\", \"bullet_points\", \"question_answer\"]\n",
    "        if analysis_type not in valid_types:\n",
    "            raise HTTPException(400, f\"Invalid analysis_type. Must be one of: {valid_types}\")\n",
    "        \n",
    "        # Build prompt based on analysis type\n",
    "        prompt, max_tokens = build_analysis_prompt(text, analysis_type, keywords)\n",
    "        \n",
    "        # Generate analysis\n",
    "        raw_output = generate_analysis(prompt, max_tokens)\n",
    "        logger.info(f\"Raw output length: {len(raw_output)}\")\n",
    "        \n",
    "        # Parse and clean the output\n",
    "        cleaned_output = parse_model_output(raw_output, prompt, analysis_type)\n",
    "        logger.info(f\"Cleaned output length: {len(cleaned_output)}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": cleaned_output,\n",
    "            \"analysis_type\": analysis_type,\n",
    "            \"keywords\": keywords,\n",
    "            \"text_length\": len(text)\n",
    "        }\n",
    "    \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis error: {e}\")\n",
    "        raise HTTPException(500, f\"Analysis failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START SERVER WITH NGROK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_port():\n",
    "    with socket.socket() as s:\n",
    "        s.bind(('', 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "port = get_port()\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "url = ngrok.connect(port).public_url\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ KAGGLE ENHANCED ANALYSIS SERVER IS READY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ”— KAGGLE URL: {url}\")\n",
    "print(f\"ðŸ”‘ API KEY: {API_KEY}\")\n",
    "print(\"âœ¨ NEW: Advanced output parser to remove prompt repetition\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“‹ ANALYSIS MODES:\")\n",
    "print(\"  â€¢ brief - Quick 2-3 sentence summary\")\n",
    "print(\"  â€¢ detailed - Comprehensive analysis\")\n",
    "print(\"  â€¢ main_topics - Extract and explain main topics\")\n",
    "print(\"  â€¢ keywords_focus - Focus on user keywords\")\n",
    "print(\"  â€¢ bullet_points - Key points in bullets\")\n",
    "print(\"  â€¢ question_answer - Q&A format summary\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "threading.Thread(\n",
    "    target=lambda: __import__('uvicorn').run(app, host=\"0.0.0.0\", port=port),\n",
    "    daemon=True\n",
    ").start()\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
