{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:26:01.215691Z",
     "iopub.status.busy": "2025-11-28T12:26:01.215002Z",
     "iopub.status.idle": "2025-11-28T12:27:21.064300Z",
     "shell.execute_reply": "2025-11-28T12:27:21.063600Z",
     "shell.execute_reply.started": "2025-11-28T12:26:01.215664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.52.4 pyngrok fastapi uvicorn python-multipart openai-whisper pypdf ffmpeg-python --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:27:21.065994Z",
     "iopub.status.busy": "2025-11-28T12:27:21.065728Z",
     "iopub.status.idle": "2025-11-28T12:27:29.478707Z",
     "shell.execute_reply": "2025-11-28T12:27:29.478094Z",
     "shell.execute_reply.started": "2025-11-28T12:27:21.065970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, torch, threading, time, socket, tempfile, logging, re\n",
    "from pyngrok import ngrok, conf\n",
    "from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Depends\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import whisper\n",
    "from pypdf import PdfReader\n",
    "from pathlib import Path\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:27:29.480005Z",
     "iopub.status.busy": "2025-11-28T12:27:29.479426Z",
     "iopub.status.idle": "2025-11-28T12:27:29.484377Z",
     "shell.execute_reply": "2025-11-28T12:27:29.483702Z",
     "shell.execute_reply.started": "2025-11-28T12:27:29.479981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "# ========================\n",
    "NGROK_TOKEN = \"your_ngrok_token_here\"\n",
    "API_KEY = \"123456\"\n",
    "TEMP_DIR = tempfile.mkdtemp()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:27:29.486495Z",
     "iopub.status.busy": "2025-11-28T12:27:29.486047Z",
     "iopub.status.idle": "2025-11-28T12:27:29.497829Z",
     "shell.execute_reply": "2025-11-28T12:27:29.497238Z",
     "shell.execute_reply.started": "2025-11-28T12:27:29.486476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# MODELS\n",
    "# ========================\n",
    "class Models:\n",
    "    mistral_model = None\n",
    "    mistral_tokenizer = None\n",
    "    whisper_model = None\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls):\n",
    "        logger.info(\"ðŸ”„ Loading Kaggle models...\")\n",
    "\n",
    "        # Load Mistral\n",
    "        logger.info(\"Loading Mistral...\")\n",
    "        cls.mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-Nemo-Instruct-2407\")\n",
    "        cls.mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True\n",
    "        ).eval()\n",
    "        logger.info(\"âœ… Mistral loaded\")\n",
    "\n",
    "        # Load Whisper (tiny to avoid GPU crash)\n",
    "        logger.info(\"Loading Whisper...\")\n",
    "        cls.whisper_model = whisper.load_model(\"tiny\")\n",
    "        logger.info(\"âœ… Whisper loaded\")\n",
    "\n",
    "        logger.info(\"âœ… All Kaggle models ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:27:29.498832Z",
     "iopub.status.busy": "2025-11-28T12:27:29.498464Z",
     "iopub.status.idle": "2025-11-28T12:27:29.508482Z",
     "shell.execute_reply": "2025-11-28T12:27:29.507800Z",
     "shell.execute_reply.started": "2025-11-28T12:27:29.498815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# SECURITY\n",
    "# ========================\n",
    "security = HTTPBearer()\n",
    "\n",
    "def verify(creds: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    if creds.credentials != API_KEY:\n",
    "        raise HTTPException(401, \"Unauthorized\")\n",
    "    return creds.credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:27:29.509570Z",
     "iopub.status.busy": "2025-11-28T12:27:29.509245Z",
     "iopub.status.idle": "2025-11-28T12:27:29.519500Z",
     "shell.execute_reply": "2025-11-28T12:27:29.518826Z",
     "shell.execute_reply.started": "2025-11-28T12:27:29.509526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# OUTPUT PARSER\n",
    "# ========================\n",
    "def parse_model_output(raw_output: str, prompt: str, analysis_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse and clean the model output to remove prompt repetition and extract only the response.\n",
    "    \"\"\"\n",
    "    # Remove the prompt from the output if it appears at the beginning\n",
    "    output = raw_output\n",
    "    \n",
    "    # Try to find where the actual response starts\n",
    "    # Common markers that indicate the start of the actual response\n",
    "    markers = {\n",
    "        \"brief\": [\"Provide a brief summary\", \"Brief summary:\", \"Summary:\"],\n",
    "        \"detailed\": [\"Detailed Summary:\", \"Detailed Analysis:\", \"Analysis:\"],\n",
    "        \"main_topics\": [\"Main Topics:\", \"Topics:\"],\n",
    "        \"keywords_focus\": [\"Keyword-Focused Analysis:\", \"Analysis:\"],\n",
    "        \"bullet_points\": [\"Key Points:\", \"â€¢\", \"-\"],\n",
    "        \"question_answer\": [\"Q&A Summary:\", \"Q:\", \"Question:\"]\n",
    "    }\n",
    "    \n",
    "    # Find the last occurrence of the marker for this analysis type\n",
    "    relevant_markers = markers.get(analysis_type, [\"Analysis:\", \"Summary:\"])\n",
    "       \n",
    "    # Try each marker\n",
    "    for marker in relevant_markers:\n",
    "        if marker in output:\n",
    "            # Split at the last occurrence of the marker\n",
    "            parts = output.split(marker)\n",
    "            if len(parts) > 1:\n",
    "                # Take everything after the last marker\n",
    "                output = parts[-1].strip()\n",
    "                break\n",
    "    \n",
    "    # Remove the prompt text if it's still there\n",
    "    prompt_lines = prompt.split('\\n')\n",
    "    for line in prompt_lines:\n",
    "        if len(line.strip()) > 20:  # Only remove substantial lines\n",
    "            output = output.replace(line, '')\n",
    "    \n",
    "    # Remove common instruction patterns using regex\n",
    "    patterns_to_remove = [\n",
    "        r'User Keywords:.*?TEXT:',\n",
    "        r'Provide a brief summary.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Provide a detailed summary.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Identify and explain.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Create a Q&A summary.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Analyze the text focusing.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Extract and explain.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'For each topic:.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'Each bullet should be.*?(?=\\n\\n|\\n[A-Z])',\n",
    "        r'\\*\\s*Topic name.*?(?=\\n)',\n",
    "        r'\\*\\s*Brief explanation.*?(?=\\n)',\n",
    "        r'\\*\\s*Key points.*?(?=\\n)',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        output = re.sub(pattern, '', output, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Clean up extra whitespace and newlines\n",
    "    output = re.sub(r'\\n{3,}', '\\n\\n', output)\n",
    "    output = output.strip()\n",
    "    \n",
    "    # If output is too short or empty, return a fallback message\n",
    "    if len(output) < 50:\n",
    "        return \"Analysis generated. Please note: The model output was too brief or could not be properly parsed.\"\n",
    "    \n",
    "    return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:31:54.464615Z",
     "iopub.status.busy": "2025-11-28T12:31:54.464256Z",
     "iopub.status.idle": "2025-11-28T12:31:54.473828Z",
     "shell.execute_reply": "2025-11-28T12:31:54.473080Z",
     "shell.execute_reply.started": "2025-11-28T12:31:54.464589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# HELPERS\n",
    "# ========================\n",
    "async def save_file(file: UploadFile, prefix: str) -> str:\n",
    "    path = os.path.join(TEMP_DIR, f\"{prefix}{Path(file.filename).suffix}\")\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(await file.read())\n",
    "    return path\n",
    "\n",
    "def extract_pdf(path: str) -> str:\n",
    "    return \"\\n\".join([p.extract_text() or \"\" for p in PdfReader(path).pages])\n",
    "\n",
    "def transcribe(path: str) -> str:\n",
    "    return Models.whisper_model.transcribe(path)[\"text\"]\n",
    "\n",
    "def extract_audio_from_video(video_path: str, audio_path: str):\n",
    "    import subprocess\n",
    "    subprocess.run([\n",
    "        'ffmpeg', '-i', video_path, '-vn', '-acodec',\n",
    "        'libmp3lame', '-q:a', '2', audio_path, '-y'\n",
    "    ], check=True, capture_output=True)\n",
    "\n",
    "def generate_analysis(prompt: str, max_len: int = 400) -> str:\n",
    "    inputs = Models.mistral_tokenizer(\n",
    "        prompt, return_tensors=\"pt\",\n",
    "        truncation=True, max_length=2048\n",
    "    ).to(Models.mistral_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = Models.mistral_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_len,\n",
    "            pad_token_id=Models.mistral_tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    return Models.mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def build_analysis_prompt(text: str, analysis_type: str, keywords: str = \"\") -> tuple:\n",
    "    \"\"\"Build analysis prompt based on type and return (prompt, max_tokens)\"\"\"\n",
    "    \n",
    "    text_preview = text[:3000]  # Limit text for context\n",
    "    keyword_text = f\"Focus Keywords: {keywords}\" if keywords else \"No specific keywords provided\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"brief\": (f\"\"\"You are a summarization expert. Analyze this text and provide ONLY a brief 2-3 sentence summary.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Write your brief summary below (2-3 sentences only):\n",
    "\"\"\", 200),\n",
    "        \n",
    "        \"detailed\": (f\"\"\"You are an analysis expert. Provide a comprehensive analysis of this text.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Write a detailed analysis covering main themes, key points, evidence, and conclusions:\n",
    "\"\"\", 600),\n",
    "        \n",
    "        \"main_topics\": (f\"\"\"You are a content analyzer. Extract the main topics from this text.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "List and explain each main topic with its key points:\n",
    "\"\"\", 500),\n",
    "        \n",
    "        \"keywords_focus\": (f\"\"\"You are an expert analyzer. Focus your analysis on these specific keywords: {keywords}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Analyze how these keywords appear in the text, their context, and significance:\n",
    "\"\"\", 500),\n",
    "        \n",
    "        \"bullet_points\": (f\"\"\"You are a summarization expert. Extract the key points as clear bullet points.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "List the main points as bullet points:\n",
    "\"\"\", 400),\n",
    "        \n",
    "        \"question_answer\": (f\"\"\"You are an expert analyzer. Create a Q&A format summary.\n",
    "\n",
    "{keyword_text}\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "{text_preview}\n",
    "\n",
    "Create questions and answers about this text:\n",
    "\"\"\", 500)\n",
    "    }\n",
    "    \n",
    "    return prompts.get(analysis_type, prompts[\"detailed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:32:00.844246Z",
     "iopub.status.busy": "2025-11-28T12:32:00.843946Z",
     "iopub.status.idle": "2025-11-28T12:32:00.859615Z",
     "shell.execute_reply": "2025-11-28T12:32:00.859000Z",
     "shell.execute_reply.started": "2025-11-28T12:32:00.844208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89/2503106960.py:6: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# FASTAPI APP\n",
    "# ========================\n",
    "app = FastAPI(title=\"Kaggle Enhanced Analysis API\", version=\"2.0\")\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    Models.load()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"status\":\"online\", \"version\": \"2.0\", \"features\": [\"extraction\", \"multi-mode analysis\", \"output parser\"]}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"models\": {\n",
    "            \"mistral\": Models.mistral_model is not None,\n",
    "            \"whisper\": Models.whisper_model is not None\n",
    "        },\n",
    "        \"analysis_modes\": [\"brief\", \"detailed\", \"main_topics\", \"keywords_focus\", \"bullet_points\", \"question_answer\"]\n",
    "    }\n",
    "\n",
    "@app.post(\"/extract\")\n",
    "async def extract(\n",
    "    input_type: str = Form(...),\n",
    "    file: UploadFile = File(...),\n",
    "    token: str = Depends(verify)\n",
    "):\n",
    "    file_path = audio_path = None\n",
    "\n",
    "    try:\n",
    "        if input_type == \"pdf\":\n",
    "            file_path = await save_file(file, \"doc\")\n",
    "            text = extract_pdf(file_path)\n",
    "\n",
    "        elif input_type == \"audio\":\n",
    "            file_path = await save_file(file, \"audio\")\n",
    "            text = transcribe(file_path)\n",
    "\n",
    "        elif input_type == \"video\":\n",
    "            file_path = await save_file(file, \"video\")\n",
    "            audio_path = os.path.join(TEMP_DIR, \"audio.mp3\")\n",
    "            extract_audio_from_video(file_path, audio_path)\n",
    "            text = transcribe(audio_path)\n",
    "\n",
    "        else:\n",
    "            raise HTTPException(400, \"Invalid input_type\")\n",
    "\n",
    "        return {\"status\":\"success\", \"text\":text, \"length\": len(text)}\n",
    "\n",
    "    finally:\n",
    "        for p in [file_path, audio_path]:\n",
    "            if p and os.path.exists(p):\n",
    "                os.remove(p)\n",
    "@app.post(\"/analyze\")\n",
    "async def analyze(\n",
    "    text: str = Form(...),\n",
    "    analysis_type: str = Form(\"detailed\"),\n",
    "    keywords: str = Form(\"\"),\n",
    "    token: str = Depends(verify)\n",
    "):\n",
    "    \"\"\"Enhanced analysis with multiple modes and output parsing\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Analysis request - Type: {analysis_type}, Keywords: {keywords}\")\n",
    "        \n",
    "        # Validate analysis type\n",
    "        valid_types = [\"brief\", \"detailed\", \"main_topics\", \"keywords_focus\", \"bullet_points\", \"question_answer\"]\n",
    "        if analysis_type not in valid_types:\n",
    "            raise HTTPException(400, f\"Invalid analysis_type. Must be one of: {valid_types}\")\n",
    "        \n",
    "        # Build prompt based on analysis type\n",
    "        prompt, max_tokens = build_analysis_prompt(text, analysis_type, keywords)\n",
    "        \n",
    "        # Generate analysis\n",
    "        raw_output = generate_analysis(prompt, max_tokens)\n",
    "        logger.info(f\"Raw output length: {len(raw_output)}\")\n",
    "        \n",
    "        # Parse and clean the output\n",
    "        cleaned_output = parse_model_output(raw_output, prompt, analysis_type)\n",
    "        logger.info(f\"Cleaned output length: {len(cleaned_output)}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"analysis\": cleaned_output,\n",
    "            \"analysis_type\": analysis_type,\n",
    "            \"keywords\": keywords,\n",
    "            \"text_length\": len(text)\n",
    "        }\n",
    "    \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis error: {e}\")\n",
    "        raise HTTPException(500, f\"Analysis failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T12:32:06.845820Z",
     "iopub.status.busy": "2025-11-28T12:32:06.845503Z",
     "iopub.status.idle": "2025-11-28T12:41:31.172034Z",
     "shell.execute_reply": "2025-11-28T12:41:31.171123Z",
     "shell.execute_reply.started": "2025-11-28T12:32:06.845797Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.ngrok:Opening tunnel named: http-53347-06de9f13-a932-4bea-a95b-176a0fd57c55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process:Overriding default auth token\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"no configuration paths supplied\"\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"FIPS 140 mode\" enabled=false\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=8bcb6ced032cc696\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=8bcb6ced032cc696 status=200 dur=262.825Âµs\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=bbb3acbf25a6074a\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=bbb3acbf25a6074a status=200 dur=97.904Âµs\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=01bc2f4cc3d19c92\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=01bc2f4cc3d19c92 status=200 dur=79.02Âµs\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=96ec73b7180f99c2\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-53347-06de9f13-a932-4bea-a95b-176a0fd57c55 addr=http://localhost:53347 url=https://isaac-thermolabile-deonna.ngrok-free.dev\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=96ec73b7180f99c2 status=201 dur=228.848147ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ KAGGLE ENHANCED ANALYSIS SERVER IS READY!\n",
      "======================================================================\n",
      "ðŸ”— KAGGLE URL: https://isaac-thermolabile-deonna.ngrok-free.dev\n",
      "ðŸ”‘ API KEY: 123456\n",
      "âœ¨ NEW: Advanced output parser to remove prompt repetition\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‹ ANALYSIS MODES:\n",
      "  â€¢ brief - Quick 2-3 sentence summary\n",
      "  â€¢ detailed - Comprehensive analysis\n",
      "  â€¢ main_topics - Extract and explain main topics\n",
      "  â€¢ keywords_focus - Focus on user keywords\n",
      "  â€¢ bullet_points - Key points in bullets\n",
      "  â€¢ question_answer - Q&A format summary\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [89]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:__main__:ðŸ”„ Loading Kaggle models...\n",
      "INFO:__main__:Loading Mistral...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cfa7b0fe0c4a90b3e881c3b2216716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95449bbeb68840a593e5f817b34e5e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa02d4c8af2d416786403d11d4342434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62f665671624475b5e29f7dfa8f2daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 12:32:15.228655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764333135.422386     160 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764333135.469144     160 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620bfe81fba04ad8ab7e1405dc488142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07db3bfb85d4e89a2e85d466c5b3f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3974616df8164a9f932b33bd4cb369fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a16f122bd443ab8de9ac86c9795566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacee8e04ca84f688bf2d6e3e9d4d84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f65568d856f40018e5cc82d985f69ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e3daca0fee4fabafff43d05d703948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe76d86cff047d9aebf2db204bdeed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137c25c71f944e6094810937ca019017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:âœ… Mistral loaded\n",
      "INFO:__main__:Loading Whisper...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.1M/72.1M [00:00<00:00, 102MiB/s]\n",
      "INFO:__main__:âœ… Whisper loaded\n",
      "INFO:__main__:âœ… All Kaggle models ready!\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:53347 (Press CTRL+C to quit)\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:31+0000 lvl=info msg=\"join connections\" obj=join id=7a29a401d195 l=127.0.0.1:53347 r=156.203.167.177:35359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:32+0000 lvl=info msg=\"join connections\" obj=join id=e88a840a9708 l=127.0.0.1:53347 r=156.203.167.177:37158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:40+0000 lvl=info msg=\"join connections\" obj=join id=1109b747925a l=127.0.0.1:53347 r=156.203.167.177:37164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:41+0000 lvl=info msg=\"join connections\" obj=join id=cf8127191414 l=127.0.0.1:53347 r=156.203.167.177:44233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:27+0000 lvl=info msg=\"join connections\" obj=join id=72cd6cb28a47 l=127.0.0.1:53347 r=156.203.167.177:37230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:28+0000 lvl=info msg=\"join connections\" obj=join id=5b283b08b939 l=127.0.0.1:53347 r=156.203.167.177:46168\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:43+0000 lvl=info msg=\"join connections\" obj=join id=5671daf694c1 l=127.0.0.1:53347 r=156.203.167.177:35411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:44+0000 lvl=info msg=\"join connections\" obj=join id=79637303d2b3 l=127.0.0.1:53347 r=156.203.167.177:44287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:46+0000 lvl=info msg=\"join connections\" obj=join id=054537c1b403 l=127.0.0.1:53347 r=156.203.167.177:44289\n",
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:47+0000 lvl=info msg=\"join connections\" obj=join id=a5fab3b6b1ba l=127.0.0.1:53347 r=156.203.167.177:44291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:48+0000 lvl=info msg=\"join connections\" obj=join id=91755865ed77 l=127.0.0.1:53347 r=156.203.167.177:44293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"POST /extract HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:40:13+0000 lvl=info msg=\"join connections\" obj=join id=4ebabee1772a l=127.0.0.1:53347 r=156.203.167.177:37304\n",
      "INFO:__main__:Analysis request - Type: detailed, Keywords: \n",
      "INFO:__main__:Raw output length: 3492\n",
      "INFO:__main__:Cleaned output length: 2216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"POST /analyze HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:41:20+0000 lvl=info msg=\"join connections\" obj=join id=6addce41e3e8 l=127.0.0.1:53347 r=156.203.167.177:44345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:41:21+0000 lvl=info msg=\"join connections\" obj=join id=4f19c6c85fc9 l=127.0.0.1:53347 r=156.203.167.177:37342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyngrok.process.ngrok:t=2025-11-28T12:41:30+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89/3938220560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# START SERVER WITH NGROK\n",
    "# ========================\n",
    "def get_port():\n",
    "    with socket.socket() as s:\n",
    "        s.bind(('', 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "port = get_port()\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "url = ngrok.connect(port).public_url\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ KAGGLE ENHANCED ANALYSIS SERVER IS READY!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"ðŸ”— KAGGLE URL: {url}\")\n",
    "print(f\"ðŸ”‘ API KEY: {API_KEY}\")\n",
    "print(\"âœ¨ NEW: Advanced output parser to remove prompt repetition\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“‹ ANALYSIS MODES:\")\n",
    "print(\"  â€¢ brief - Quick 2-3 sentence summary\")\n",
    "print(\"  â€¢ detailed - Comprehensive analysis\")\n",
    "print(\"  â€¢ main_topics - Extract and explain main topics\")\n",
    "print(\"  â€¢ keywords_focus - Focus on user keywords\")\n",
    "print(\"  â€¢ bullet_points - Key points in bullets\")\n",
    "print(\"  â€¢ question_answer - Q&A format summary\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "threading.Thread(\n",
    "    target=lambda: __import__('uvicorn').run(app, host=\"0.0.0.0\", port=port),\n",
    "    daemon=True\n",
    ").start()\n",
    "\n",
    "while True:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
