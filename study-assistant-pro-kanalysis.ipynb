{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.52.4 pyngrok fastapi uvicorn python-multipart openai-whisper pypdf ffmpeg-python --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:26:01.215002Z","iopub.execute_input":"2025-11-28T12:26:01.215691Z","iopub.status.idle":"2025-11-28T12:27:21.064300Z","shell.execute_reply.started":"2025-11-28T12:26:01.215664Z","shell.execute_reply":"2025-11-28T12:27:21.063600Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport os, torch, threading, time, socket, tempfile, logging, re\nfrom pyngrok import ngrok, conf\nfrom fastapi import FastAPI, UploadFile, File, Form, HTTPException, Depends\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport whisper\nfrom pypdf import PdfReader\nfrom pathlib import Path\nfrom typing import Optional","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:27:21.065728Z","iopub.execute_input":"2025-11-28T12:27:21.065994Z","iopub.status.idle":"2025-11-28T12:27:29.478707Z","shell.execute_reply.started":"2025-11-28T12:27:21.065970Z","shell.execute_reply":"2025-11-28T12:27:29.478094Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# CONFIG\n# ========================\nNGROK_TOKEN = \"363Knh4iSz4E2Q8RNeF3PYPdafA_85edGzbDmyWZR9iGWWwA2\"\nAPI_KEY = \"123456\"\nTEMP_DIR = tempfile.mkdtemp()\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:27:29.479426Z","iopub.execute_input":"2025-11-28T12:27:29.480005Z","iopub.status.idle":"2025-11-28T12:27:29.484377Z","shell.execute_reply.started":"2025-11-28T12:27:29.479981Z","shell.execute_reply":"2025-11-28T12:27:29.483702Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ========================\n# MODELS\n# ========================\nclass Models:\n    mistral_model = None\n    mistral_tokenizer = None\n    whisper_model = None\n\n    @classmethod\n    def load(cls):\n        logger.info(\"ðŸ”„ Loading Kaggle models...\")\n\n        # Load Mistral\n        logger.info(\"Loading Mistral...\")\n        cls.mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-Nemo-Instruct-2407\")\n        cls.mistral_model = AutoModelForCausalLM.from_pretrained(\n            \"mistralai/Mistral-Nemo-Instruct-2407\",\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True\n        ).eval()\n        logger.info(\"âœ… Mistral loaded\")\n\n        # Load Whisper (tiny to avoid GPU crash)\n        logger.info(\"Loading Whisper...\")\n        cls.whisper_model = whisper.load_model(\"tiny\")\n        logger.info(\"âœ… Whisper loaded\")\n\n        logger.info(\"âœ… All Kaggle models ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:27:29.486047Z","iopub.execute_input":"2025-11-28T12:27:29.486495Z","iopub.status.idle":"2025-11-28T12:27:29.497829Z","shell.execute_reply.started":"2025-11-28T12:27:29.486476Z","shell.execute_reply":"2025-11-28T12:27:29.497238Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ========================\n# SECURITY\n# ========================\nsecurity = HTTPBearer()\n\ndef verify(creds: HTTPAuthorizationCredentials = Depends(security)):\n    if creds.credentials != API_KEY:\n        raise HTTPException(401, \"Unauthorized\")\n    return creds.credentials\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:27:29.498464Z","iopub.execute_input":"2025-11-28T12:27:29.498832Z","iopub.status.idle":"2025-11-28T12:27:29.508482Z","shell.execute_reply.started":"2025-11-28T12:27:29.498815Z","shell.execute_reply":"2025-11-28T12:27:29.507800Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ========================\n# OUTPUT PARSER\n# ========================\ndef parse_model_output(raw_output: str, prompt: str, analysis_type: str) -> str:\n    \"\"\"\n    Parse and clean the model output to remove prompt repetition and extract only the response.\n    \"\"\"\n    # Remove the prompt from the output if it appears at the beginning\n    output = raw_output\n    \n    # Try to find where the actual response starts\n    # Common markers that indicate the start of the actual response\n    markers = {\n        \"brief\": [\"Provide a brief summary\", \"Brief summary:\", \"Summary:\"],\n        \"detailed\": [\"Detailed Summary:\", \"Detailed Analysis:\", \"Analysis:\"],\n        \"main_topics\": [\"Main Topics:\", \"Topics:\"],\n        \"keywords_focus\": [\"Keyword-Focused Analysis:\", \"Analysis:\"],\n        \"bullet_points\": [\"Key Points:\", \"â€¢\", \"-\"],\n        \"question_answer\": [\"Q&A Summary:\", \"Q:\", \"Question:\"]\n    }\n    \n    # Find the last occurrence of the marker for this analysis type\n    relevant_markers = markers.get(analysis_type, [\"Analysis:\", \"Summary:\"])\n       \n    # Try each marker\n    for marker in relevant_markers:\n        if marker in output:\n            # Split at the last occurrence of the marker\n            parts = output.split(marker)\n            if len(parts) > 1:\n                # Take everything after the last marker\n                output = parts[-1].strip()\n                break\n    \n    # Remove the prompt text if it's still there\n    prompt_lines = prompt.split('\\n')\n    for line in prompt_lines:\n        if len(line.strip()) > 20:  # Only remove substantial lines\n            output = output.replace(line, '')\n    \n    # Remove common instruction patterns using regex\n    patterns_to_remove = [\n        r'User Keywords:.*?TEXT:',\n        r'Provide a brief summary.*?(?=\\n\\n|\\n[A-Z])',\n        r'Provide a detailed summary.*?(?=\\n\\n|\\n[A-Z])',\n        r'Identify and explain.*?(?=\\n\\n|\\n[A-Z])',\n        r'Create a Q&A summary.*?(?=\\n\\n|\\n[A-Z])',\n        r'Analyze the text focusing.*?(?=\\n\\n|\\n[A-Z])',\n        r'Extract and explain.*?(?=\\n\\n|\\n[A-Z])',\n        r'For each topic:.*?(?=\\n\\n|\\n[A-Z])',\n        r'Each bullet should be.*?(?=\\n\\n|\\n[A-Z])',\n        r'\\*\\s*Topic name.*?(?=\\n)',\n        r'\\*\\s*Brief explanation.*?(?=\\n)',\n        r'\\*\\s*Key points.*?(?=\\n)',\n    ]\n    \n    for pattern in patterns_to_remove:\n        output = re.sub(pattern, '', output, flags=re.DOTALL | re.IGNORECASE)\n    \n    # Clean up extra whitespace and newlines\n    output = re.sub(r'\\n{3,}', '\\n\\n', output)\n    output = output.strip()\n    \n    # If output is too short or empty, return a fallback message\n    if len(output) < 50:\n        return \"Analysis generated. Please note: The model output was too brief or could not be properly parsed.\"\n    \n    return output\n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:27:29.509245Z","iopub.execute_input":"2025-11-28T12:27:29.509570Z","iopub.status.idle":"2025-11-28T12:27:29.519500Z","shell.execute_reply.started":"2025-11-28T12:27:29.509526Z","shell.execute_reply":"2025-11-28T12:27:29.518826Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ========================\n# HELPERS\n# ========================\nasync def save_file(file: UploadFile, prefix: str) -> str:\n    path = os.path.join(TEMP_DIR, f\"{prefix}{Path(file.filename).suffix}\")\n    with open(path, 'wb') as f:\n        f.write(await file.read())\n    return path\n\ndef extract_pdf(path: str) -> str:\n    return \"\\n\".join([p.extract_text() or \"\" for p in PdfReader(path).pages])\n\ndef transcribe(path: str) -> str:\n    return Models.whisper_model.transcribe(path)[\"text\"]\n\ndef extract_audio_from_video(video_path: str, audio_path: str):\n    import subprocess\n    subprocess.run([\n        'ffmpeg', '-i', video_path, '-vn', '-acodec',\n        'libmp3lame', '-q:a', '2', audio_path, '-y'\n    ], check=True, capture_output=True)\n\ndef generate_analysis(prompt: str, max_len: int = 400) -> str:\n    inputs = Models.mistral_tokenizer(\n        prompt, return_tensors=\"pt\",\n        truncation=True, max_length=2048\n    ).to(Models.mistral_model.device)\n\n    with torch.no_grad():\n        outputs = Models.mistral_model.generate(\n            **inputs,\n            max_new_tokens=max_len,\n            pad_token_id=Models.mistral_tokenizer.eos_token_id,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n\n    return Models.mistral_tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ndef build_analysis_prompt(text: str, analysis_type: str, keywords: str = \"\") -> tuple:\n    \"\"\"Build analysis prompt based on type and return (prompt, max_tokens)\"\"\"\n    \n    text_preview = text[:3000]  # Limit text for context\n    keyword_text = f\"Focus Keywords: {keywords}\" if keywords else \"No specific keywords provided\"\n    \n    prompts = {\n        \"brief\": (f\"\"\"You are a summarization expert. Analyze this text and provide ONLY a brief 2-3 sentence summary.\n\n{keyword_text}\n\nTEXT TO ANALYZE:\n{text_preview}\n\nWrite your brief summary below (2-3 sentences only):\n\"\"\", 200),\n        \n        \"detailed\": (f\"\"\"You are an analysis expert. Provide a comprehensive analysis of this text.\n\n{keyword_text}\n\nTEXT TO ANALYZE:\n{text_preview}\n\nWrite a detailed analysis covering main themes, key points, evidence, and conclusions:\n\"\"\", 600),\n        \n        \"main_topics\": (f\"\"\"You are a content analyzer. Extract the main topics from this text.\n\n{keyword_text}\n\nTEXT TO ANALYZE:\n{text_preview}\n\nList and explain each main topic with its key points:\n\"\"\", 500),\n        \n        \"keywords_focus\": (f\"\"\"You are an expert analyzer. Focus your analysis on these specific keywords: {keywords}\n\nTEXT TO ANALYZE:\n{text_preview}\n\nAnalyze how these keywords appear in the text, their context, and significance:\n\"\"\", 500),\n        \n        \"bullet_points\": (f\"\"\"You are a summarization expert. Extract the key points as clear bullet points.\n\n{keyword_text}\n\nTEXT TO ANALYZE:\n{text_preview}\n\nList the main points as bullet points:\n\"\"\", 400),\n        \n        \"question_answer\": (f\"\"\"You are an expert analyzer. Create a Q&A format summary.\n\n{keyword_text}\n\nTEXT TO ANALYZE:\n{text_preview}\n\nCreate questions and answers about this text:\n\"\"\", 500)\n    }\n    \n    return prompts.get(analysis_type, prompts[\"detailed\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:31:54.464256Z","iopub.execute_input":"2025-11-28T12:31:54.464615Z","iopub.status.idle":"2025-11-28T12:31:54.473828Z","shell.execute_reply.started":"2025-11-28T12:31:54.464589Z","shell.execute_reply":"2025-11-28T12:31:54.473080Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ========================\n# FASTAPI APP\n# ========================\napp = FastAPI(title=\"Kaggle Enhanced Analysis API\", version=\"2.0\")\n\n@app.on_event(\"startup\")\nasync def startup():\n    Models.load()\n\n@app.get(\"/\")\nasync def root():\n    return {\"status\":\"online\", \"version\": \"2.0\", \"features\": [\"extraction\", \"multi-mode analysis\", \"output parser\"]}\n\n@app.get(\"/health\")\nasync def health():\n    return {\n        \"status\": \"healthy\",\n        \"models\": {\n            \"mistral\": Models.mistral_model is not None,\n            \"whisper\": Models.whisper_model is not None\n        },\n        \"analysis_modes\": [\"brief\", \"detailed\", \"main_topics\", \"keywords_focus\", \"bullet_points\", \"question_answer\"]\n    }\n\n@app.post(\"/extract\")\nasync def extract(\n    input_type: str = Form(...),\n    file: UploadFile = File(...),\n    token: str = Depends(verify)\n):\n    file_path = audio_path = None\n\n    try:\n        if input_type == \"pdf\":\n            file_path = await save_file(file, \"doc\")\n            text = extract_pdf(file_path)\n\n        elif input_type == \"audio\":\n            file_path = await save_file(file, \"audio\")\n            text = transcribe(file_path)\n\n        elif input_type == \"video\":\n            file_path = await save_file(file, \"video\")\n            audio_path = os.path.join(TEMP_DIR, \"audio.mp3\")\n            extract_audio_from_video(file_path, audio_path)\n            text = transcribe(audio_path)\n\n        else:\n            raise HTTPException(400, \"Invalid input_type\")\n\n        return {\"status\":\"success\", \"text\":text, \"length\": len(text)}\n\n    finally:\n        for p in [file_path, audio_path]:\n            if p and os.path.exists(p):\n                os.remove(p)\n@app.post(\"/analyze\")\nasync def analyze(\n    text: str = Form(...),\n    analysis_type: str = Form(\"detailed\"),\n    keywords: str = Form(\"\"),\n    token: str = Depends(verify)\n):\n    \"\"\"Enhanced analysis with multiple modes and output parsing\"\"\"\n    try:\n        logger.info(f\"Analysis request - Type: {analysis_type}, Keywords: {keywords}\")\n        \n        # Validate analysis type\n        valid_types = [\"brief\", \"detailed\", \"main_topics\", \"keywords_focus\", \"bullet_points\", \"question_answer\"]\n        if analysis_type not in valid_types:\n            raise HTTPException(400, f\"Invalid analysis_type. Must be one of: {valid_types}\")\n        \n        # Build prompt based on analysis type\n        prompt, max_tokens = build_analysis_prompt(text, analysis_type, keywords)\n        \n        # Generate analysis\n        raw_output = generate_analysis(prompt, max_tokens)\n        logger.info(f\"Raw output length: {len(raw_output)}\")\n        \n        # Parse and clean the output\n        cleaned_output = parse_model_output(raw_output, prompt, analysis_type)\n        logger.info(f\"Cleaned output length: {len(cleaned_output)}\")\n        \n        return {\n            \"status\": \"success\",\n            \"analysis\": cleaned_output,\n            \"analysis_type\": analysis_type,\n            \"keywords\": keywords,\n            \"text_length\": len(text)\n        }\n    \n    except HTTPException:\n        raise\n    except Exception as e:\n        logger.error(f\"Analysis error: {e}\")\n        raise HTTPException(500, f\"Analysis failed: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:32:00.843946Z","iopub.execute_input":"2025-11-28T12:32:00.844246Z","iopub.status.idle":"2025-11-28T12:32:00.859615Z","shell.execute_reply.started":"2025-11-28T12:32:00.844208Z","shell.execute_reply":"2025-11-28T12:32:00.859000Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_89/2503106960.py:6: DeprecationWarning: \n        on_event is deprecated, use lifespan event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n        \n  @app.on_event(\"startup\")\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ========================\n# START SERVER WITH NGROK\n# ========================\ndef get_port():\n    with socket.socket() as s:\n        s.bind(('', 0))\n        return s.getsockname()[1]\n\nport = get_port()\nconf.get_default().auth_token = NGROK_TOKEN\nurl = ngrok.connect(port).public_url\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŽ‰ KAGGLE ENHANCED ANALYSIS SERVER IS READY!\")\nprint(\"=\"*70)\nprint(f\"ðŸ”— KAGGLE URL: {url}\")\nprint(f\"ðŸ”‘ API KEY: {API_KEY}\")\nprint(\"âœ¨ NEW: Advanced output parser to remove prompt repetition\")\nprint(\"=\"*70)\nprint(\"\\nðŸ“‹ ANALYSIS MODES:\")\nprint(\"  â€¢ brief - Quick 2-3 sentence summary\")\nprint(\"  â€¢ detailed - Comprehensive analysis\")\nprint(\"  â€¢ main_topics - Extract and explain main topics\")\nprint(\"  â€¢ keywords_focus - Focus on user keywords\")\nprint(\"  â€¢ bullet_points - Key points in bullets\")\nprint(\"  â€¢ question_answer - Q&A format summary\")\nprint(\"=\"*70 + \"\\n\")\n\nthreading.Thread(\n    target=lambda: __import__('uvicorn').run(app, host=\"0.0.0.0\", port=port),\n    daemon=True\n).start()\n\nwhile True:\n    time.sleep(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:32:06.845503Z","iopub.execute_input":"2025-11-28T12:32:06.845820Z","iopub.status.idle":"2025-11-28T12:41:31.172034Z","shell.execute_reply.started":"2025-11-28T12:32:06.845797Z","shell.execute_reply":"2025-11-28T12:41:31.171123Z"}},"outputs":[{"name":"stderr","text":"INFO:pyngrok.ngrok:Opening tunnel named: http-53347-06de9f13-a932-4bea-a95b-176a0fd57c55\n","output_type":"stream"},{"name":"stdout","text":"                                                                                                    \r","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process:Overriding default auth token\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"no configuration paths supplied\"\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"FIPS 140 mode\" enabled=false\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"client session established\" obj=tunnels.session\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=8bcb6ced032cc696\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=8bcb6ced032cc696 status=200 dur=262.825Âµs\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=bbb3acbf25a6074a\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=bbb3acbf25a6074a status=200 dur=97.904Âµs\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=01bc2f4cc3d19c92\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=01bc2f4cc3d19c92 status=200 dur=79.02Âµs\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=start pg=/api/tunnels id=96ec73b7180f99c2\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=http-53347-06de9f13-a932-4bea-a95b-176a0fd57c55 addr=http://localhost:53347 url=https://isaac-thermolabile-deonna.ngrok-free.dev\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:32:07+0000 lvl=info msg=end pg=/api/tunnels id=96ec73b7180f99c2 status=201 dur=228.848147ms\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nðŸŽ‰ KAGGLE ENHANCED ANALYSIS SERVER IS READY!\n======================================================================\nðŸ”— KAGGLE URL: https://isaac-thermolabile-deonna.ngrok-free.dev\nðŸ”‘ API KEY: 123456\nâœ¨ NEW: Advanced output parser to remove prompt repetition\n======================================================================\n\nðŸ“‹ ANALYSIS MODES:\n  â€¢ brief - Quick 2-3 sentence summary\n  â€¢ detailed - Comprehensive analysis\n  â€¢ main_topics - Extract and explain main topics\n  â€¢ keywords_focus - Focus on user keywords\n  â€¢ bullet_points - Key points in bullets\n  â€¢ question_answer - Q&A format summary\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [89]\nINFO:     Waiting for application startup.\nINFO:__main__:ðŸ”„ Loading Kaggle models...\nINFO:__main__:Loading Mistral...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cfa7b0fe0c4a90b3e881c3b2216716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95449bbeb68840a593e5f817b34e5e3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa02d4c8af2d416786403d11d4342434"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62f665671624475b5e29f7dfa8f2daa"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 12:32:15.228655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764333135.422386     160 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764333135.469144     160 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"620bfe81fba04ad8ab7e1405dc488142"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e07db3bfb85d4e89a2e85d466c5b3f2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3974616df8164a9f932b33bd4cb369fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2a16f122bd443ab8de9ac86c9795566"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aacee8e04ca84f688bf2d6e3e9d4d84a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f65568d856f40018e5cc82d985f69ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3e3daca0fee4fabafff43d05d703948"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe76d86cff047d9aebf2db204bdeed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"137c25c71f944e6094810937ca019017"}},"metadata":{}},{"name":"stderr","text":"INFO:__main__:âœ… Mistral loaded\nINFO:__main__:Loading Whisper...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72.1M/72.1M [00:00<00:00, 102MiB/s]\nINFO:__main__:âœ… Whisper loaded\nINFO:__main__:âœ… All Kaggle models ready!\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:53347 (Press CTRL+C to quit)\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:38:31+0000 lvl=info msg=\"join connections\" obj=join id=7a29a401d195 l=127.0.0.1:53347 r=156.203.167.177:35359\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:32+0000 lvl=info msg=\"join connections\" obj=join id=e88a840a9708 l=127.0.0.1:53347 r=156.203.167.177:37158\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:40+0000 lvl=info msg=\"join connections\" obj=join id=1109b747925a l=127.0.0.1:53347 r=156.203.167.177:37164\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:38:41+0000 lvl=info msg=\"join connections\" obj=join id=cf8127191414 l=127.0.0.1:53347 r=156.203.167.177:44233\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:27+0000 lvl=info msg=\"join connections\" obj=join id=72cd6cb28a47 l=127.0.0.1:53347 r=156.203.167.177:37230\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:28+0000 lvl=info msg=\"join connections\" obj=join id=5b283b08b939 l=127.0.0.1:53347 r=156.203.167.177:46168\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:39:43+0000 lvl=info msg=\"join connections\" obj=join id=5671daf694c1 l=127.0.0.1:53347 r=156.203.167.177:35411\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\nINFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:44+0000 lvl=info msg=\"join connections\" obj=join id=79637303d2b3 l=127.0.0.1:53347 r=156.203.167.177:44287\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:46+0000 lvl=info msg=\"join connections\" obj=join id=054537c1b403 l=127.0.0.1:53347 r=156.203.167.177:44289\nINFO:pyngrok.process.ngrok:t=2025-11-28T12:39:47+0000 lvl=info msg=\"join connections\" obj=join id=a5fab3b6b1ba l=127.0.0.1:53347 r=156.203.167.177:44291\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:39:48+0000 lvl=info msg=\"join connections\" obj=join id=91755865ed77 l=127.0.0.1:53347 r=156.203.167.177:44293\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"POST /extract HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:40:13+0000 lvl=info msg=\"join connections\" obj=join id=4ebabee1772a l=127.0.0.1:53347 r=156.203.167.177:37304\nINFO:__main__:Analysis request - Type: detailed, Keywords: \nINFO:__main__:Raw output length: 3492\nINFO:__main__:Cleaned output length: 2216\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"POST /analyze HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:41:20+0000 lvl=info msg=\"join connections\" obj=join id=6addce41e3e8 l=127.0.0.1:53347 r=156.203.167.177:44345\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:41:21+0000 lvl=info msg=\"join connections\" obj=join id=4f19c6c85fc9 l=127.0.0.1:53347 r=156.203.167.177:37342\n","output_type":"stream"},{"name":"stdout","text":"INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n","output_type":"stream"},{"name":"stderr","text":"INFO:pyngrok.process.ngrok:t=2025-11-28T12:41:30+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_89/3938220560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}