{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCVQowlVX3-F",
        "outputId": "3548a370-fdeb-466f-ff23-e12be462e760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Installing Ollama...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pyngrok fastapi uvicorn python-multipart sentence-transformers faiss-cpu ollama --quiet\n",
        "\n",
        "# Install Ollama\n",
        "print(\"ðŸ“¥ Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxa6da7LUI_X",
        "outputId": "9e7ac72a-456b-4438-9d0d-7d8477c48743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting Ollama server...\n",
            "ðŸ“¥ Downloading TinyLlama model via Ollama...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "âœ… Model downloaded!\n"
          ]
        }
      ],
      "source": [
        "# Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "print(\"ðŸš€ Starting Ollama server...\")\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "time.sleep(5)\n",
        "\n",
        "# Pull TinyLlama model\n",
        "print(\"ðŸ“¥ Downloading TinyLlama model via Ollama...\")\n",
        "!ollama pull tinyllama\n",
        "print(\"âœ… Model downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLfQv-bXYAGG"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, threading, socket, logging\n",
        "from typing import List\n",
        "from pyngrok import ngrok, conf\n",
        "from fastapi import FastAPI, Form, HTTPException, Depends\n",
        "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcdKGl3LYAO0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ========================\n",
        "# CONFIGURATION - CHANGE YOUR TOKEN HERE\n",
        "# ========================\n",
        "NGROK_TOKEN = \"your_first_ngrok_token_here\"  # Get from https://dashboard.ngrok.com\n",
        "API_KEY = \"123456\"  # Must match Streamlit and Kaggle\n",
        "CHUNK_SIZE = 400\n",
        "CHUNK_OVERLAP = 40\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhNrnSWbYARf"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# MODELS\n",
        "# ========================\n",
        "class Models:\n",
        "    embedder = None\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls):\n",
        "        logger.info(\"ðŸ”„ Loading Colab RAG models...\")\n",
        "\n",
        "        # Load embedding model\n",
        "        logger.info(\"Loading embeddings...\")\n",
        "        cls.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        logger.info(\"âœ… Embedder loaded\")\n",
        "\n",
        "        # Test Ollama connection\n",
        "        try:\n",
        "            ollama.list()\n",
        "            logger.info(\"âœ… Ollama connected with TinyLlama\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ollama connection failed: {e}\")\n",
        "            raise\n",
        "\n",
        "        logger.info(\"âœ… All Colab RAG models ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1PeAMg_YAUQ"
      },
      "outputs": [],
      "source": [
        "# SECURITY\n",
        "# ========================\n",
        "security = HTTPBearer()\n",
        "\n",
        "def verify(creds: HTTPAuthorizationCredentials = Depends(security)):\n",
        "    if creds.credentials != API_KEY:\n",
        "        raise HTTPException(401, \"Unauthorized\")\n",
        "    return creds.credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW_69llDYAXC"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# RAG FUNCTIONS\n",
        "# ========================\n",
        "def chunk_text(text: str) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks\"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) < CHUNK_SIZE:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), CHUNK_SIZE - CHUNK_OVERLAP):\n",
        "        chunk = \" \".join(words[i:i + CHUNK_SIZE])\n",
        "        if len(chunk.split()) > 20:  # Minimum chunk size\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks if chunks else [text]\n",
        "\n",
        "def build_index(chunks: List[str]):\n",
        "    \"\"\"Build FAISS index from chunks\"\"\"\n",
        "    embeddings = Models.embedder.encode(chunks, show_progress_bar=False)\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    return index\n",
        "\n",
        "def retrieve(query: str, index, chunks: List[str], k: int = 3) -> List[str]:\n",
        "    \"\"\"Retrieve top-k relevant chunks\"\"\"\n",
        "    query_embedding = Models.embedder.encode([query], show_progress_bar=False).astype('float32')\n",
        "    _, indices = index.search(query_embedding, min(k, len(chunks)))\n",
        "    return [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "\n",
        "def generate_answer(prompt: str) -> str:\n",
        "    \"\"\"Generate answer with Ollama TinyLlama\"\"\"\n",
        "    try:\n",
        "        response = ollama.generate(\n",
        "            model='tinyllama',\n",
        "            prompt=prompt,\n",
        "            options={\n",
        "                'temperature': 0.3,\n",
        "                'num_predict': 250,\n",
        "                'stop': ['\\n\\n', 'Question:', 'Context:']\n",
        "            }\n",
        "        )\n",
        "        return response['response'].strip()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ollama generation error: {e}\")\n",
        "        raise HTTPException(500, f\"Answer generation failed: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4ICN6-YYAZn",
        "outputId": "34a14abd-50e6-4ce7-ff84-a0cc49b1082c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-124282178.py:6: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @app.on_event(\"startup\")\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# FASTAPI APP\n",
        "# ========================\n",
        "app = FastAPI(title=\"Colab RAG API with Ollama\", version=\"1.0\")\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup():\n",
        "    Models.load()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\n",
        "        \"status\": \"online\",\n",
        "        \"service\": \"Colab RAG Server (Ollama)\",\n",
        "        \"capabilities\": [\"rag\", \"question_answering\"],\n",
        "        \"model\": \"tinyllama via Ollama\"\n",
        "    }\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"server\": \"colab\",\n",
        "        \"models\": {\n",
        "            \"embedder\": Models.embedder is not None,\n",
        "            \"ollama\": \"tinyllama\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.post(\"/rag\")\n",
        "async def rag(\n",
        "    text: str = Form(...),\n",
        "    question: str = Form(...),\n",
        "    token: str = Depends(verify)\n",
        "):\n",
        "    \"\"\"Answer question using RAG pipeline with Ollama\"\"\"\n",
        "    try:\n",
        "        if not question or not text:\n",
        "            raise HTTPException(400, \"Both text and question are required\")\n",
        "\n",
        "        logger.info(f\"Processing RAG query: {question[:50]}...\")\n",
        "\n",
        "        # Step 1: Chunk the text\n",
        "        chunks = chunk_text(text)\n",
        "        logger.info(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "        # Step 2: Build search index\n",
        "        index = build_index(chunks)\n",
        "\n",
        "        # Step 3: Retrieve relevant chunks\n",
        "        sources = retrieve(question, index, chunks, k=3)\n",
        "        logger.info(f\"Retrieved {len(sources)} relevant chunks\")\n",
        "\n",
        "        # Step 4: Build context\n",
        "        context = \"\\n\\n\".join([f\"[{i+1}] {chunk[:300]}\" for i, chunk in enumerate(sources)])\n",
        "\n",
        "        # Step 5: Generate answer with Ollama\n",
        "        prompt = f\"\"\"Based on the following context, answer the question concisely and accurately.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        answer = generate_answer(prompt)\n",
        "        logger.info(\"âœ… Answer generated with Ollama\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"answer\": answer,\n",
        "            \"sources\": sources,\n",
        "            \"num_chunks\": len(chunks),\n",
        "            \"question\": question,\n",
        "            \"model\": \"tinyllama (Ollama)\"\n",
        "        }\n",
        "\n",
        "    except HTTPException:\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logger.error(f\"RAG error: {e}\")\n",
        "        raise HTTPException(500, f\"RAG processing failed: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJfsEJxNYlp-",
        "outputId": "9a1c90a5-85ac-4562-f993-15fd28305a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸŽ‰ COLAB RAG SERVER IS READY (USING OLLAMA)!\n",
            "======================================================================\n",
            "ðŸ”— COLAB URL: https://simply-nonpersistent-irena.ngrok-free.dev\n",
            "ðŸ”‘ API KEY: 123456\n",
            "ðŸ¤– MODEL: TinyLlama via Ollama\n",
            "======================================================================\n",
            "\n",
            "ðŸ“‹ COPY THE URL ABOVE TO YOUR STREAMLIT APP!\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [790]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:41585 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "INFO:     156.203.167.177:0 - \"GET /health HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "# START SERVER WITH NGROK\n",
        "# ========================\n",
        "def get_port():\n",
        "    with socket.socket() as s:\n",
        "        s.bind(('', 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "# Setup ngrok\n",
        "port = get_port()\n",
        "conf.get_default().auth_token = NGROK_TOKEN\n",
        "url = ngrok.connect(port).public_url\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ‰ COLAB RAG SERVER IS READY (USING OLLAMA)!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"ðŸ”— COLAB URL: {url}\")\n",
        "print(f\"ðŸ”‘ API KEY: {API_KEY}\")\n",
        "print(f\"ðŸ¤– MODEL: TinyLlama via Ollama\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nðŸ“‹ COPY THE URL ABOVE TO YOUR STREAMLIT APP!\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Start FastAPI server\n",
        "threading.Thread(\n",
        "    target=lambda: __import__('uvicorn').run(app, host=\"0.0.0.0\", port=port),\n",
        "    daemon=True\n",
        ").start()\n",
        "\n",
        "# Keep running\n",
        "logger.info(\"âœ… Colab RAG server with Ollama is running!\")\n",
        "while True:\n",
        "    time.sleep(1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
